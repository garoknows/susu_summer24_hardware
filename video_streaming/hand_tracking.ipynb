{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Landmarks Definition: Landmarks are specific points identified on a hand. In MediaPipe's context, these are key points such as fingertips, knuckles, and the palm center.\n",
    "\n",
    "Coordinates: Each landmark is represented by its (x, y, z) coordinates:\n",
    "    - x and y: Coordinates within the image or frame, ranging from 0 to 1.0. They indicate the position in the frame.\n",
    "    - z: Depth coordinate, indicating how far the landmark is from the camera plane. This is provided as a floating-point value.\n",
    "\n",
    "Detection and Tracking: MediaPipe uses machine learning models to detect and track these landmarks in real-time. It leverages deep learning techniques to accurately identify the positions of these points across frames.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False,\n",
    "                       max_num_hands=10,\n",
    "                       min_detection_confidence=0.5,\n",
    "                       min_tracking_confidence=0.5)\n",
    "\n",
    "# Initialize MediaPipe Drawing\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Set the video source: 0 for webcam, or provide a video file path\n",
    "video_source = \"C:/Users/MSI/Desktop/chrome_F1vsSREpWC.mp4\"\n",
    "#video_source = 0\n",
    "\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video source: {video_source}\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame from video source.\")\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Hands\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # Draw hand landmarks\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Print landmark coordinates\n",
    "            for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "                # Get landmark coordinates\n",
    "                landmark_x = int(landmark.x * frame.shape[1])\n",
    "                landmark_y = int(landmark.y * frame.shape[0])\n",
    "                landmark_z = landmark.z  # Z-coordinate (depth)\n",
    "\n",
    "                # Print coordinates of each landmark\n",
    "                print(f\"Landmark {idx}: ({landmark_x}, {landmark_y}, {landmark_z})\")\n",
    "\n",
    "                # Check the video source and obtain the file name\n",
    "                if video_source == 0:\n",
    "                    file_name = \"webcam\"\n",
    "                else:\n",
    "                    file_name = video_source.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "                # Save landmark coordinates as csv file\n",
    "                # Overwrites the file if it already exists\n",
    "\n",
    "                if not os.path.exists('hand_landmarks_' + file_name +'.csv'):\n",
    "                   with open('hand_landmarks_' + file_name +'.csv', 'w') as f:\n",
    "                    f.write(f\"Landmark_{idx}, {landmark_x},{landmark_y},{landmark_z}\\n\")\n",
    "\n",
    "                # Draw circles on the landmarks (optional)\n",
    "                cv2.circle(frame, (landmark_x, landmark_y), 5, (255, 0, 0), -1)\n",
    "\n",
    "            # Draw hand landmarks on the frame\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Hand Detection', frame)\n",
    "\n",
    "    # Exit on 'q' key press or window close\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q') or cv2.getWindowProperty('Hand Detection', cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### real time hand detection using rtsp stream with mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mBad file descriptor (C:\\Users\\runneradmin\\AppData\\Local\\Temp\\tmpw8fxpxar\\build\\_deps\\bundled_libzmq-src\\src\\epoll.cpp:73). \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# real-time hand detection using rtsp stream with mediapipe \n",
    "# single feed\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "# Replace with your RTSP stream URL\n",
    "rtsp_url = \"rtsp://4kkzxW:hDneHFEeidTc@192.168.1.123:554/live/ch1\"\n",
    "#rtsp_url= \"rtsp://TK1Xnf:LbAiQiGLPvRd@192.168.1.174:554/live/ch1\"\n",
    "#rtsp_url=\"rtsp://UmZF6h:atAIz1ecLgC8@192.168.1.127:554/live/ch1\"\n",
    "\n",
    "\n",
    "# Initialize MediaPipe hands and drawing utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Connect to the RTSP stream\n",
    "cap = cv2.VideoCapture(rtsp_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video stream\")\n",
    "    exit()\n",
    "\n",
    "# Initialize variables for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7) as hands:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, image = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to read frame\")\n",
    "            break\n",
    "\n",
    "        # Calculate FPS\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "\n",
    "        # Convert the FPS to an integer\n",
    "        fps = int(fps)\n",
    "\n",
    "        # Convert the frame rate to a string\n",
    "        fps_text = \"FPS: \" + str(fps)\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display\n",
    "        # Convert the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display the frame rate on the image\n",
    "        cv2.putText(image, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"feed: 01\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('RTSP Stream with MediaPipe Hands and FPS', image)\n",
    "\n",
    "        # Press 'q' to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi feed stream with cli and recording option with start and end time\n",
    "\n",
    "to run: \n",
    "python hand_detection_rtsp.py --streams \"1:rtsp://your_rtsp_url1,2:rtsp://your_rtsp_url2\" --start \"10:00 AM\" --end \"03:00 PM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\MSI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --streams STREAMS [--start START]\n",
      "                             [--end END]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --streams\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "# Define function to convert AM/PM times to seconds since midnight\n",
    "def time_to_seconds(time_str):\n",
    "    dt = datetime.strptime(time_str, \"%I:%M %p\")\n",
    "    return dt.hour * 3600 + dt.minute * 60\n",
    "\n",
    "# Define function to parse arguments\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Real-time hand detection using RTSP streams with MediaPipe.\")\n",
    "    parser.add_argument('--streams', type=str, required=True, \n",
    "                        help=\"Comma-separated list of RTSP stream URLs in the format 'feed_number:url'.\")\n",
    "    parser.add_argument('--start', type=str, default=None, \n",
    "                        help=\"Start time for recording in AM/PM format (e.g., '10:00 AM').\")\n",
    "    parser.add_argument('--end', type=str, default=None, \n",
    "                        help=\"End time for recording in AM/PM format (e.g., '03:00 PM').\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "# Parse command line arguments\n",
    "args = parse_arguments()\n",
    "\n",
    "# Convert stream URLs from arguments\n",
    "rtsp_streams = {}\n",
    "for stream in args.streams.split(','):\n",
    "    key, url = stream.split(':')\n",
    "    rtsp_streams[key] = url\n",
    "\n",
    "# Convert start and end times to seconds\n",
    "start_time_seconds = time_to_seconds(args.start) if args.start else None\n",
    "end_time_seconds = time_to_seconds(args.end) if args.end else None\n",
    "\n",
    "# Initialize MediaPipe hands and drawing utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Connect to the RTSP streams\n",
    "caps = {key: cv2.VideoCapture(url) for key, url in rtsp_streams.items()}\n",
    "\n",
    "if not all(cap.isOpened() for cap in caps.values()):\n",
    "    print(\"Error: Unable to open one or more video streams\")\n",
    "    exit()\n",
    "\n",
    "# Initialize variables for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "# Initialize video writers\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out_videos = {}\n",
    "\n",
    "# Flags for recording\n",
    "is_recording = False\n",
    "record_start_time = None\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7) as hands:\n",
    "    \n",
    "    start_program_time = time.time()\n",
    "    while all(cap.isOpened() for cap in caps.values()):\n",
    "        frames = {}\n",
    "        for key, cap in caps.items():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Error: Unable to read frame from feed {key}\")\n",
    "                break\n",
    "            frames[key] = frame\n",
    "        \n",
    "        if len(frames) != len(caps):\n",
    "            break\n",
    "\n",
    "        # Calculate FPS\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "\n",
    "        # Convert the FPS to an integer\n",
    "        fps = int(fps)\n",
    "\n",
    "        # Convert the frame rate to a string\n",
    "        fps_text = \"FPS: \" + str(fps)\n",
    "\n",
    "        processed_frames = []\n",
    "        for key, image in frames.items():\n",
    "            # Process the image\n",
    "            image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = hands.process(image)\n",
    "\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            cv2.putText(image, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, f\"feed: {key}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            processed_frames.append(image)\n",
    "\n",
    "            # Write the frame to the video file if recording\n",
    "            if is_recording:\n",
    "                if key not in out_videos:\n",
    "                    out_videos[key] = cv2.VideoWriter(f'feed_{key}.avi', fourcc, 20.0, (image.shape[1], image.shape[0]))\n",
    "                out_videos[key].write(image)\n",
    "\n",
    "        # Concatenate images horizontally\n",
    "        combined_image = cv2.hconcat(processed_frames)\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('RTSP Streams with MediaPipe Hands and FPS', combined_image)\n",
    "\n",
    "        # Handle key events\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('r'):\n",
    "            is_recording = not is_recording\n",
    "            if is_recording:\n",
    "                record_start_time = time.time()\n",
    "                print(\"Recording started\")\n",
    "            else:\n",
    "                for out in out_videos.values():\n",
    "                    out.release()\n",
    "                out_videos.clear()\n",
    "                print(\"Recording stopped\")\n",
    "                record_start_time = None\n",
    "\n",
    "        # Automatic start and stop based on provided start and end times\n",
    "        current_time = time.time() - start_program_time\n",
    "        if start_time_seconds is not None and end_time_seconds is not None:\n",
    "            if current_time >= start_time_seconds and current_time < end_time_seconds and not is_recording:\n",
    "                is_recording = True\n",
    "                record_start_time = time.time()\n",
    "                print(\"Recording started automatically at start time\")\n",
    "            elif current_time >= end_time_seconds and is_recording:\n",
    "                is_recording = False\n",
    "                for out in out_videos.values():\n",
    "                    out.release()\n",
    "                out_videos.clear()\n",
    "                print(\"Recording stopped automatically at end time\")\n",
    "                record_start_time = None\n",
    "\n",
    "# Release all resources\n",
    "for cap in caps.values():\n",
    "    cap.release()\n",
    "for out in out_videos.values():\n",
    "    out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without argeparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Define dictionary with multiple RTSP stream URLs\n",
    "rtsp_streams = {\n",
    "    'feed1': 'rtsp://UmZF6h:atAIz1ecLgC8@192.168.1.127:554/live/ch1',\n",
    "    'feed2': 'rtsp://TK1Xnf:LbAiQiGLPvRd@192.168.1.174:554/live/ch1',\n",
    "    'feed3': 'rtsp://4kkzxW:hDneHFEeidTc@192.168.1.123:554/live/ch1'\n",
    "}\n",
    "\n",
    "# Initialize MediaPipe hands and drawing utilities\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Connect to the RTSP streams\n",
    "caps = {key: cv2.VideoCapture(url) for key, url in rtsp_streams.items()}\n",
    "\n",
    "if not all(cap.isOpened() for cap in caps.values()):\n",
    "    print(\"Error: Unable to open one or more video streams\")\n",
    "    exit()\n",
    "\n",
    "# Initialize variables for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.7) as hands:\n",
    "    \n",
    "    while all(cap.isOpened() for cap in caps.values()):\n",
    "        frames = {}\n",
    "        for key, cap in caps.items():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Error: Unable to read frame from feed {key}\")\n",
    "                break\n",
    "            frames[key] = frame\n",
    "        \n",
    "        if len(frames) != len(caps):\n",
    "            break\n",
    "\n",
    "        # Calculate FPS\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "\n",
    "        # Convert the FPS to an integer\n",
    "        fps = int(fps)\n",
    "\n",
    "        # Convert the frame rate to a string\n",
    "        fps_text = \"FPS: \" + str(fps)\n",
    "\n",
    "        processed_frames = []\n",
    "        for key, image in frames.items():\n",
    "            # Process the image\n",
    "            image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = hands.process(image)\n",
    "\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            cv2.putText(image, fps_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image, f\"feed: {key}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            processed_frames.append(image)\n",
    "\n",
    "        # Ensure we have exactly 4 frames for the 2x2 grid\n",
    "        blank_frame = np.zeros_like(processed_frames[0])\n",
    "        while len(processed_frames) < 4:\n",
    "            processed_frames.append(blank_frame)\n",
    "\n",
    "        # Arrange frames in a 2x2 grid\n",
    "        top_row = cv2.hconcat([processed_frames[0], processed_frames[1]])\n",
    "        bottom_row = cv2.hconcat([processed_frames[2], processed_frames[3]])\n",
    "        combined_image = cv2.vconcat([top_row, bottom_row])\n",
    "\n",
    "        # Display the combined frame\n",
    "        cv2.imshow('RTSP Streams with MediaPipe Hands and FPS', combined_image)\n",
    "\n",
    "        # Handle key events\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release all resources\n",
    "for cap in caps.values():\n",
    "    cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
